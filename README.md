# Sharif University of technology Course on Vision and Language

## Introduction
### What is a Vision-Language Model (VLM)?

A [VLM](https://huggingface.co/blog/vlms) is a model that can process and understand both visual and textual data. It bridges the gap between computer vision and natural language processing, enabling tasks like image captioning, visual question answering, and image-text retrieval.

### What is CLIP?

[CLIP](https://openai.com/index/clip/) (Contrastive Languageâ€“Image Pre-training) is a model developed by OpenAI that learns visual concepts from natural language supervision. It can understand images and associate them with textual descriptions. You can explore CLIP models on [HuggingFace Models](https://huggingface.co/models?other=clip).

### What is an LLM?

A [Large Language Model (LLM)](https://en.wikipedia.org/wiki/Large_language_model) is a neural network trained on vast amounts of text data to understand and generate human-like text. Examples include OpenAI's GPT-4, [GPT-2](https://huggingface.co/openai-community/gpt2), and Google's [T5](https://huggingface.co/google-t5/t5-base).

## Prerequisites

  - Basic understanding of Python and PyTorch.
  - Familiarity with machine learning concepts.
  - Installed Python 3.7 or higher.
  - A NVIDIA GPU.

## Project Structure

This Course is divided into a series of Jupyter Notebooks, each focusing on fundamental analysis and concepts in LLM and VLM.
